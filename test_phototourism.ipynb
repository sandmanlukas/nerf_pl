{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils import load_ckpt, visualize_depth\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "\n",
    "import metrics\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from datasets import dataset_dict\n",
    "import json\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your settings...\n",
    "############################\n",
    "N_vocab = 800\n",
    "encode_appearance = True\n",
    "N_a = 48\n",
    "encode_transient = True\n",
    "N_tau = 16\n",
    "beta_min = 0.1 # doesn't have effect in testing\n",
    "\n",
    "# TODO: change to correct optimization ckpt\n",
    "ckpt_path = '/home/lukas/exjobb-nerf/nerf_pl/ckpts/IC_scale3_nerfw_correct_cam_params/last.ckpt'\n",
    "embedding_ckpt_path = '/home/lukas/exjobb-nerf/ckpts/IC_scale3_nerfw_optimize_appearance_top_bottom/last.ckpt'\n",
    "\n",
    "N_emb_xyz = 10\n",
    "N_emb_dir = 4\n",
    "N_samples = 256\n",
    "N_importance = 256\n",
    "use_disp = False\n",
    "chunk = 1024*32\n",
    "#############################\n",
    "\n",
    "embedding_xyz = PosEmbedding(N_emb_xyz-1, N_emb_xyz)\n",
    "embedding_dir = PosEmbedding(N_emb_dir-1, N_emb_dir)\n",
    "embeddings = {'xyz': embedding_xyz, 'dir': embedding_dir}\n",
    "\n",
    "if encode_appearance:\n",
    "    embedding_a = torch.nn.Embedding(N_vocab, N_a).cuda()\n",
    "    # load embedding from optimization\n",
    "    load_ckpt(embedding_a, embedding_ckpt_path, model_name='embedding_a')\n",
    "    embeddings['a'] = embedding_a\n",
    "\n",
    "if encode_transient:\n",
    "    embedding_t = torch.nn.Embedding(N_vocab, N_tau).cuda()\n",
    "    load_ckpt(embedding_t, ckpt_path, model_name='embedding_t')\n",
    "    embeddings['t'] = embedding_t\n",
    "    \n",
    "\n",
    "nerf_coarse = NeRF('coarse',\n",
    "                   in_channels_xyz=6*N_emb_xyz+3,\n",
    "                   in_channels_dir=6*N_emb_dir+3,\n",
    "                   encode_appearance=encode_appearance\n",
    "                   ).cuda()\n",
    "\n",
    "nerf_fine = NeRF('fine',\n",
    "                 in_channels_xyz=6*N_emb_xyz+3,\n",
    "                 in_channels_dir=6*N_emb_dir+3,\n",
    "                 encode_appearance=encode_appearance,\n",
    "                 in_channels_a=N_a,\n",
    "                 encode_transient=encode_transient,\n",
    "                 in_channels_t=N_tau,\n",
    "                 beta_min=beta_min).cuda()\n",
    "\n",
    "load_ckpt(nerf_coarse, ckpt_path, model_name='nerf_coarse')\n",
    "load_ckpt(nerf_fine, ckpt_path, model_name='nerf_fine')\n",
    "\n",
    "models = {'coarse': nerf_coarse, 'fine': nerf_fine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def f(rays, ts, **kwargs):\n",
    "    \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
    "    B = rays.shape[0]\n",
    "    results = defaultdict(list)\n",
    "    for i in range(0, B, chunk):\n",
    "        kwargs_ = {}\n",
    "        if 'a_embedded' in kwargs:\n",
    "            kwargs_['a_embedded'] = kwargs['a_embedded'][i:i+chunk]\n",
    "        rendered_ray_chunks = \\\n",
    "            render_rays(models,\n",
    "                        embeddings,\n",
    "                        rays[i:i+chunk],\n",
    "                        ts[i:i+chunk],\n",
    "                        N_samples,\n",
    "                        use_disp,\n",
    "                        0,\n",
    "                        0,\n",
    "                        N_importance,\n",
    "                        chunk,\n",
    "                        dataset.white_back,\n",
    "                        test_time=True,\n",
    "                        **kwargs_)\n",
    "\n",
    "        for k, v in rendered_ray_chunks.items():\n",
    "            results[k] += [v]\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = torch.cat(v, 0)\n",
    "    del rendered_ray_chunks\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jupyter/data/IC_patient_000/segment_001/datasets/'\n",
    "\n",
    "dataset = dataset_dict['phototourism'] \\\n",
    "          (root_dir,\n",
    "           split='test_train',\n",
    "           img_downscale=3, \n",
    "           use_cache=False, \n",
    "           tsv_file='IC_scale3_nerfw.tsv',\n",
    "           exp_name='eval_IC_scale3_nerfw_top_bottom',\n",
    "           mask_path=f'{root_dir}/mask.png',\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "00785.png\n",
      "00749.png\n",
      "00714.png\n",
      "00683.png\n",
      "00646.png\n",
      "00613.png\n",
      "00580.png\n",
      "00548.png\n",
      "00521.png\n",
      "00491.png\n",
      "00462.png\n",
      "00433.png\n",
      "00404.png\n",
      "00164.png\n",
      "00134.png\n",
      "00102.png\n",
      "00028.png\n",
      "00042.png\n",
      "00074.png\n",
      "00206.png\n",
      "00240.png\n",
      "00272.png\n",
      "00301.png\n",
      "00327.png\n",
      "00359.png\n"
     ]
    }
   ],
   "source": [
    "img_ids_test = dataset.img_ids_test\n",
    "print(len(img_ids_test))\n",
    "for idx in img_ids_test:\n",
    "    print(dataset.image_paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating results for id: 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 15.89 GiB of which 1.95 GiB is free. Process 2156156 has 11.47 GiB memory in use. Including non-PyTorch memory, this process has 2.47 GiB memory in use. Of the allocated memory 2.15 GiB is allocated by PyTorch, and 31.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m sample[\u001b[39m\"\u001b[39m\u001b[39mrgbs_top\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m rgbs_top\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m samples[idx] \u001b[39m=\u001b[39m sample\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m results[idx] \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mcpu() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m (f(rays, ts))\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39ma_embedded\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     kwargs_[\u001b[39m'\u001b[39m\u001b[39ma_embedded\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39ma_embedded\u001b[39m\u001b[39m'\u001b[39m][i:i\u001b[39m+\u001b[39mchunk]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m rendered_ray_chunks \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     render_rays(models,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                 embeddings,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                 rays[i:i\u001b[39m+\u001b[39;49mchunk],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m                 ts[i:i\u001b[39m+\u001b[39;49mchunk],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m                 N_samples,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m                 use_disp,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                 \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m                 \u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m                 N_importance,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m                 chunk,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m                 dataset\u001b[39m.\u001b[39;49mwhite_back,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m                 test_time\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m rendered_ray_chunks\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnerf-vm.europe-west1-b.tenfifty-exjobb/home/lukas/exjobb-nerf/nerf_pl/test_phototourism.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     results[k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [v]\n",
      "File \u001b[0;32m~/exjobb-nerf/nerf_pl/models/rendering.py:333\u001b[0m, in \u001b[0;36mrender_rays\u001b[0;34m(models, embeddings, rays, ts, N_samples, use_disp, perturb, noise_std, N_importance, chunk, white_back, test_time, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m             t_embedded \u001b[39m=\u001b[39m embeddings[\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m](ts)\n\u001b[0;32m--> 333\u001b[0m     inference(results, model, xyz_fine, z_vals, test_time, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/exjobb-nerf/nerf_pl/models/rendering.py:122\u001b[0m, in \u001b[0;36mrender_rays.<locals>.inference\u001b[0;34m(results, model, xyz, z_vals, test_time, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# create other necessary inputs\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mencode_appearance:\n\u001b[0;32m--> 122\u001b[0m     a_embedded_ \u001b[39m=\u001b[39m repeat(a_embedded, \u001b[39m\"\u001b[39;49m\u001b[39mn1 c -> (n1 n2) c\u001b[39;49m\u001b[39m\"\u001b[39;49m, n2\u001b[39m=\u001b[39;49mN_samples_)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m output_transient:\n\u001b[1;32m    124\u001b[0m     t_embedded_ \u001b[39m=\u001b[39m repeat(t_embedded, \u001b[39m\"\u001b[39m\u001b[39mn1 c -> (n1 n2) c\u001b[39m\u001b[39m\"\u001b[39m, n2\u001b[39m=\u001b[39mN_samples_)\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/einops/einops.py:461\u001b[0m, in \u001b[0;36mrepeat\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat\u001b[39m(tensor, pattern, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maxes_lengths):\n\u001b[1;32m    428\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m    einops.repeat allows reordering elements and repeating them in arbitrary combinations.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m    This operation includes functionality of repeat, tile, broadcast functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39m    Find more examples in einops tutorial.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m     \u001b[39mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrepeat\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49maxes_lengths)\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/einops/einops.py:368\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    366\u001b[0m     hashable_axes_lengths \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39msorted\u001b[39m(axes_lengths\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    367\u001b[0m     recipe \u001b[39m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[39m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mreturn\u001b[39;00m recipe\u001b[39m.\u001b[39;49mapply(tensor)\n\u001b[1;32m    369\u001b[0m \u001b[39mexcept\u001b[39;00m EinopsError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m Error while processing \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m-reduction pattern \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(reduction, pattern)\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/einops/einops.py:210\u001b[0m, in \u001b[0;36mTransformRecipe.apply\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    208\u001b[0m tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mtranspose(tensor, axes_reordering)\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(added_axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49madd_axes(tensor, n_axes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(axes_reordering) \u001b[39m+\u001b[39;49m \u001b[39mlen\u001b[39;49m(added_axes), pos2len\u001b[39m=\u001b[39;49madded_axes)\n\u001b[1;32m    211\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mreshape(tensor, final_shapes)\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/einops/_backends.py:103\u001b[0m, in \u001b[0;36mAbstractBackend.add_axes\u001b[0;34m(self, x, n_axes, pos2len)\u001b[0m\n\u001b[1;32m    101\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_axis(x, axis_position)\n\u001b[1;32m    102\u001b[0m     repeats[axis_position] \u001b[39m=\u001b[39m axis_length\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtile(x, \u001b[39mtuple\u001b[39;49m(repeats))\n",
      "File \u001b[0;32m/opt/conda/envs/nerf_pl/lib/python3.9/site-packages/einops/_backends.py:337\u001b[0m, in \u001b[0;36mTorchBackend.tile\u001b[0;34m(self, x, repeats)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtile\u001b[39m(\u001b[39mself\u001b[39m, x, repeats):\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mrepeat(repeats)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 15.89 GiB of which 1.95 GiB is free. Process 2156156 has 11.47 GiB memory in use. Including non-PyTorch memory, this process has 2.47 GiB memory in use. Of the allocated memory 2.15 GiB is allocated by PyTorch, and 31.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# CHANGE TO CORRECT IMAGE\n",
    "\n",
    "results = {}\n",
    "samples = {}\n",
    "for idx in img_ids_test:\n",
    "    print(f\"Calculating results for id: {idx}...\")\n",
    "    sample = dataset[idx]\n",
    "    sample['dataset_path'] = dataset.image_paths[idx]\n",
    "    img_w, img_h = sample[\"img_wh\"]\n",
    "\n",
    "    rays = sample[\"rays\"].cuda()\n",
    "    ts = sample[\"ts\"].cuda()\n",
    "    rgbs = sample[\"rgbs\"]\n",
    "\n",
    "    rgbs = rgbs.view(img_h, img_w, -1)\n",
    "    rgbs_top, rgbs_bottom = torch.split(rgbs, [img_h // 2, img_h // 2], dim=0)\n",
    "    rgbs_top = rgbs_top.contiguous().view(-1)\n",
    "\n",
    "    sample[\"rgbs_top\"] = rgbs_top\n",
    "    samples[idx] = sample\n",
    "    results[idx] = {k: v.cpu() for k, v in (f(rays, ts)).items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dataset.mask.permute(1,2,0)\n",
    "img_w, img_h = mask.shape[0], mask.shape[1]\n",
    "\n",
    "mask_top, mask_bottom = torch.split(mask, [img_h//2, img_h//2], dim=0)\n",
    "\n",
    "plt.subplots(figsize=(15, 8))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.subplot(131)\n",
    "plt.title('mask')\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.title('mask top')\n",
    "plt.imshow(mask_top, cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.title('mask bottom')\n",
    "plt.imshow(mask_bottom, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_img(mask, img):\n",
    "    img = img if torch.is_tensor(img) else torch.from_numpy(img)\n",
    "    return img * mask\n",
    "\n",
    "def plot_print_results(sample, result,mask, encode_transient):\n",
    "    img_w, img_h = tuple(sample['img_wh'].numpy())\n",
    "\n",
    "\n",
    "    # get whole GT and top half\n",
    "    img_gt = sample['rgbs'].reshape(img_h, img_w, 3)\n",
    "    img_gt_top = sample['rgbs_top'].reshape(img_h // 2, img_w, 3)\n",
    "\n",
    "    # get whole pred and split in half\n",
    "    img_pred = result['rgb_fine'].reshape(img_h, img_w, 3)\n",
    "    img_pred_top, img_pred_bottom = torch.split(img_pred, [img_h//2, img_h//2], dim=0)\n",
    "\n",
    "    depth_pred = result['depth_fine'].reshape(img_h, img_w)\n",
    "\n",
    "    # mask GTs and preds\n",
    "    img_gt = masked_img(mask,img_gt)\n",
    "    img_gt_top = masked_img(mask_top,img_gt_top)\n",
    "\n",
    "    masked_pred = masked_img(mask,img_pred)\n",
    "    masked_pred_top = masked_img(mask_top,img_pred_top)\n",
    "\n",
    "    # save results\n",
    "    img_dir = os.path.join(root_dir, 'results', 'images')\n",
    "    depth_dir = os.path.join(root_dir, 'results', 'depth')\n",
    "\n",
    "    # create dirs if not exist\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(depth_dir, exist_ok=True)\n",
    "\n",
    "    img_name = sample['dataset_path']\n",
    "    depth_name = f'{img_name}_depth.png'\n",
    "\n",
    "    save_image(img_pred, os.path.join(img_dir, img_name))\n",
    "    save_image(depth_pred, os.path.join(dept_dir, depth_name))\n",
    "\n",
    "\n",
    "    # plot results\n",
    "    plt.subplots(figsize=(15, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(241)\n",
    "    plt.title(\"GT masked\")\n",
    "    plt.imshow(img_gt)\n",
    "\n",
    "    plt.subplot(242)\n",
    "    plt.title(\"pred\")\n",
    "    plt.imshow(img_pred)\n",
    "\n",
    "    plt.subplot(243)\n",
    "    plt.title(\"pred with mask\")\n",
    "    plt.imshow(masked_pred)\n",
    "\n",
    "    plt.subplot(244)\n",
    "    plt.title(\"depth\")\n",
    "    plt.imshow(visualize_depth(depth_pred).permute(1, 2, 0))\n",
    "\n",
    "    plt.subplot(245)\n",
    "    plt.title(\"GT top\")\n",
    "    plt.imshow(img_gt_top)\n",
    "\n",
    "    plt.subplot(246)\n",
    "    plt.title(\"pred top\")\n",
    "    plt.imshow(img_pred_top)\n",
    "\n",
    "    plt.subplot(247)\n",
    "    plt.title(\"pred top masked\")\n",
    "    plt.imshow(masked_pred_top)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # valid mask\n",
    "    valid_mask = (mask_top != 0).repeat(1,1,3)\n",
    "\n",
    "    # calculate scores on top half of image\n",
    "    psnr_ = metrics.psnr(img_gt_top, img_pred_top).item()\n",
    "    ssim_ = metrics.ssim(img_gt_top, img_pred_top).item()\n",
    "    lpips_ = metrics.lpips_score(img_gt_top, img_pred_top).item()\n",
    "\n",
    "    psnr_mask_ = metrics.psnr(img_gt_top, masked_pred_top).item()\n",
    "    psnr_unmask_ = metrics.psnr(img_gt_top, masked_pred_top, valid_mask=valid_mask).item()\n",
    "    ssim_mask_ = metrics.ssim(img_gt_top, masked_pred_top).item()\n",
    "    lpips_mask_ = metrics.lpips_score(img_gt_top, masked_pred_top).item()\n",
    "\n",
    "    print(f'{\"#\"*15} Scores for real predicition {\"#\"*15}')\n",
    "    print('PSNR between GT and pred:', psnr_)\n",
    "    print('SSIM between GT and pred:', ssim_)\n",
    "    print('LPIPS between GT and pred:',lpips_, '\\n') \n",
    "\n",
    "    print(f'{\"#\"*15} Scores for masked predicition {\"#\"*15}')\n",
    "    print('PSNR between GT and pred:', psnr_mask_)\n",
    "    print('UNMASKED PSNR between GT and pred:', psnr_unmask_)\n",
    "    print('SSIM between GT and pred:', ssim_mask_)\n",
    "    print('LPIPS between GT and pred:',lpips_mask_) \n",
    "\n",
    "\n",
    "\n",
    "    if encode_transient:\n",
    "        print('Decomposition--------------------------------------------' + \n",
    "            '---------------------------------------------------------' +\n",
    "            '---------------------------------------------------------' + \n",
    "            '---------------------------------------------------------')\n",
    "        valid_mask_static = sample['valid_mask'].reshape(img_wh[1], img_wh[0])\n",
    "        beta = result['beta'].reshape(img_wh[1], img_wh[0])\n",
    "        img_pred_static = result['rgb_fine_static'].reshape(img_wh[1], img_wh[0], 3)\n",
    "        img_pred_transient = result['_rgb_fine_transient'].reshape(img_wh[1], img_wh[0], 3)\n",
    "        depth_pred_static = result['depth_fine_static'].reshape(img_wh[1], img_wh[0])\n",
    "        depth_pred_transient = result['depth_fine_transient'].reshape(img_wh[1], img_wh[0])\n",
    "        plt.subplots(figsize=(15, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.subplot(231)\n",
    "        plt.title('static')\n",
    "        plt.imshow(img_pred_static)\n",
    "        plt.subplot(232)\n",
    "        plt.title('transient')\n",
    "        plt.imshow(img_pred_transient)\n",
    "        plt.subplot(233)\n",
    "        plt.title('uncertainty (beta)')\n",
    "        plt.imshow(beta-beta_min, cmap='gray')\n",
    "        plt.subplot(234)\n",
    "        plt.title('static depth')\n",
    "        plt.imshow(visualize_depth(depth_pred_static).permute(1,2,0))\n",
    "        plt.subplot(235)\n",
    "        plt.title('valid mask')\n",
    "        plt.imshow(valid_mask_static, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return psnr_,psnr_mask_,psnr_unmask_, ssim_, ssim_mask_, lpips_, lpips_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnrs = []\n",
    "ssims = []\n",
    "lpips_scores = []\n",
    "\n",
    "psnrs_mask = []\n",
    "psnrs_unmask = []\n",
    "ssims_mask = []\n",
    "lpips_scores_mask = []\n",
    "\n",
    "# don't evaluate on images with transient objects\n",
    "# skipped_images = [0,1,2,3]\n",
    "skipped_images = []\n",
    "for i,idx in enumerate(img_ids_test):\n",
    "     if i in skipped_images:\n",
    "          continue\n",
    "     sample = samples[idx]\n",
    "     result = results[idx]\n",
    "     \n",
    "     psnr,psnr_mask,psnr_unmask, ssim,ssim_mask, lpips, lpips_mask = plot_print_results(sample=sample, result=result,mask=mask, encode_transient=False)\n",
    "     \n",
    "     psnrs.append(psnr)\n",
    "     psnrs_unmask.append(psnr_unmask)\n",
    "     ssims.append(ssim)\n",
    "     lpips_scores.append(lpips)\n",
    "\n",
    "     psnrs_mask.append(psnr_mask)\n",
    "     ssims_mask.append(ssim_mask)\n",
    "     lpips_scores_mask.append(lpips_mask)\n",
    "\n",
    "\n",
    "# Masked\n",
    "\n",
    "# PSNR\n",
    "mean_psnr = round(np.mean(psnrs_mask),2)\n",
    "median_psnr = round(np.median(psnrs_mask),2)\n",
    "max_psnr = round(np.max(psnrs_mask),2)\n",
    "min_psnr = round(np.min(psnrs_mask),2)\n",
    "\n",
    "mean_psnr_unmask = round(np.mean(psnrs_unmask),2)\n",
    "median_psnr_unmask = round(np.median(psnrs_unmask),2)\n",
    "max_psnr_unmask = round(np.max(psnrs_unmask),2)\n",
    "min_psnr_unmask = round(np.min(psnrs_unmask),2)\n",
    "\n",
    "# SSIM\n",
    "mean_ssim = round(np.mean(ssims_mask),3)\n",
    "median_ssim = round(np.median(ssims_mask),3)\n",
    "max_ssim = round(np.max(ssims_mask),3)\n",
    "min_ssim = round(np.min(ssims_mask),3)\n",
    "\n",
    "# LPIPS\n",
    "mean_lpips = round(np.mean(lpips_mask),4)\n",
    "median_lpips = round(np.median(lpips_mask),4)\n",
    "max_lpips = round(np.max(lpips_mask),4)\n",
    "min_lpips = round(np.min(lpips_mask),4)\n",
    "\n",
    "\n",
    "print(f'{\"#\"*15} Mean scores for masked prediction {\"#\"*15}')\n",
    "print(\"Mean PSNR: \", mean_psnr)\n",
    "print(\"Mean UNMASKED PSNR: \", mean_psnr_unmask)\n",
    "print(\"Mean SSIM: \", mean_ssim)\n",
    "print(\"Mean LPIPS: \", mean_lpips)\n",
    "\n",
    "data = {\n",
    "     'mean_psnr': mean_psnr,\n",
    "     'median_psnr': median_psnr,\n",
    "     'max_psnr': max_psnr,\n",
    "     'min_psnr': min_psnr,\n",
    "     'mean_psnr_unmasked': mean_psnr_unmask,\n",
    "     'median_psnr_unmasked': median_psnr_unmask,\n",
    "     'max_psnr_unmasked': max_psnr_unmask,\n",
    "     'min_psnr_unmasked': min_psnr_unmask,\n",
    "     'mean_ssim': mean_ssim,\n",
    "     'median_ssim': median_ssim,\n",
    "     'max_ssim': max_ssim,\n",
    "     'min_ssim': min_ssim,\n",
    "     'mean_lpips': mean_lpips,\n",
    "     'median_lpips': median_lpips,\n",
    "     'max_lpips': max_lpips,\n",
    "     'min_lpips': min_lpips\n",
    "}\n",
    "\n",
    "json_obj = json.dumps(data)\n",
    "results_file = f'results_{dataset.exp_name}.json' if not skipped_images else f'results_{dataset.exp_name}_skipped_images.json'\n",
    "with open(os.path.join(root_dir,'results', results_file),'w') as file:\n",
    "     file.write(json_obj)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "#del samples\n",
    "#del results\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate embedding for appearance change (Fig 8 in the paper)\n",
    "### left image: train number 53; right image: train number 111\n",
    "### The pose is fixed to that of the right image, and the appearance embedding is interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_sample = dataset[53]\n",
    "# right_sample = dataset[111]\n",
    "\n",
    "# right_rays = right_sample['rays'].cuda()\n",
    "# right_ts = right_sample['ts'].cuda()\n",
    "# left_a_embedded = embedding_a(left_sample['ts'][0].cuda())\n",
    "# right_a_embedded = embedding_a(right_sample['ts'].cuda())\n",
    "\n",
    "# results_list = [left_sample]\n",
    "\n",
    "# for i in range(5):\n",
    "#     kwargs = {'a_embedded': right_a_embedded*i/4+left_a_embedded*(1-i/4)}\n",
    "#     results_list += [f(right_rays, right_ts, **kwargs)]\n",
    "\n",
    "# results_list += [right_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots(figsize=(20, 10))\n",
    "# for i, results in enumerate(results_list):\n",
    "#     if i == 0:\n",
    "#         img_wh = tuple(results['img_wh'].numpy())\n",
    "#         left_GT = results['rgbs'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "#         plt.subplot(241)\n",
    "#         plt.axis('off')\n",
    "#         plt.title('left GT')\n",
    "#         plt.imshow(left_GT)\n",
    "#     elif i == 6:\n",
    "#         img_wh = tuple(results['img_wh'].numpy())\n",
    "#         right_GT = results['rgbs'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "#         plt.subplot(247)\n",
    "#         plt.axis('off')\n",
    "#         plt.title('right GT')\n",
    "#         plt.imshow(right_GT)\n",
    "#     else:\n",
    "#         img_wh = tuple(right_sample['img_wh'].numpy())\n",
    "#         img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "#         plt.subplot(2, 4, i+1)\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(img_pred)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\n",
    "    os.path.join(root_dir, \"images\", '00000.png')\n",
    ").convert(\"RGB\")\n",
    "\n",
    "img_w, img_h = img.size\n",
    "img_w = img_w // 3\n",
    "img_h = img_h // 3\n",
    "img = img.resize((img_w, img_h), Image.LANCZOS)\n",
    "\n",
    "img_tensor = T.ToTensor()(img).permute(1,2,0)\n",
    "print(img_tensor.shape)\n",
    "plt.tight_layout()\n",
    "plt.subplots(figsize=(15, 8))\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(img)\n",
    "plt.subplot(132)\n",
    "plt.imshow(mask)\n",
    "plt.subplot(133)\n",
    "plt.imshow(img_tensor*mask)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
